{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zaOWfQctLsC","executionInfo":{"status":"ok","timestamp":1743852616709,"user_tz":-120,"elapsed":10114,"user":{"displayName":"Ziad Sameh","userId":"01817499041981397063"}},"outputId":"c95f5baa-c1bd-4299-c4fe-99a7cf68c9b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting memory_profiler\n","  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n","Installing collected packages: memory_profiler\n","Successfully installed memory_profiler-0.61.0\n"]}],"source":["!pip install memory_profiler psutil gymnasium ale-py tensorflow matplotlib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sNXrW9jto0z","executionInfo":{"status":"ok","timestamp":1743852648741,"user_tz":-120,"elapsed":32019,"user":{"displayName":"Ziad Sameh","userId":"01817499041981397063"}},"outputId":"513e73aa-6572-45fb-b572-5f3fe338d77a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import ale_py\n","import gymnasium as gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model, layers\n","import time\n","import gc\n","import matplotlib.pyplot as plt\n","import psutil\n","\n","class TrainingConfig:\n","    CHECKPOINT_DIR = \"/content/drive/MyDrive/ppo_pacman_models\"\n","    TOTAL_EPISODES = 1000\n","    CHECKPOINT_FREQ = 50\n","    MIN_REPLAY_HISTORY = 10000\n","\n","    def __init__(self):\n","        mem = psutil.virtual_memory()\n","        if mem.available > 12 * 1024**3:\n","            self.BATCH_SIZE = 64\n","            self.FRAME_STACK_SIZE = 4\n","            self.LEARNING_RATE = 2.5e-4\n","        else:\n","            self.BATCH_SIZE = 32\n","            self.FRAME_STACK_SIZE = 3\n","            self.LEARNING_RATE = 1e-4\n","\n","        self.GAMMA = 0.99\n","        self.LAMBDA_GAE = 0.95\n","        self.EPS_CLIP = 0.1\n","        self.UPDATE_EPOCHS = 4\n","        self.FRAME_SKIP = 4\n","        self.BETA_ENTROPY = 0.01\n","        self.VF_COEF = 0.5\n","\n","        print(f\"Using configuration (RAM available: {mem.available/1024**3:.1f}GB):\")\n","        print(f\"- Batch size: {self.BATCH_SIZE}\")\n","        print(f\"- Frame stack: {self.FRAME_STACK_SIZE}\")\n","\n","class PPOPolicy(Model):\n","    def __init__(self, action_size):\n","        super().__init__()\n","        self.conv1 = layers.Conv2D(32, (8,8), strides=4, activation='relu')\n","        self.conv2 = layers.Conv2D(64, (4,4), strides=2, activation='relu')\n","        self.conv3 = layers.Conv2D(64, (3,3), strides=1, activation='relu')\n","        self.flatten = layers.Flatten()\n","        self.dense = layers.Dense(512, activation='relu')\n","\n","        self.policy_logits = layers.Dense(action_size, activation=None)\n","        self.value = layers.Dense(1, activation=None)\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","\n","        return self.policy_logits(x), self.value(x)\n","\n","class PPOAgent:\n","    def __init__(self, config):\n","        self.config = config\n","        os.makedirs(self.config.CHECKPOINT_DIR, exist_ok=True)\n","\n","        self.env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\",\n","                            frameskip=self.config.FRAME_SKIP)\n","\n","        self.action_size = self.env.action_space.n\n","        self.policy_net = PPOPolicy(self.action_size)\n","\n","        dummy_input = tf.random.normal((1, 88, 80, self.config.FRAME_STACK_SIZE))\n","        _ = self.policy_net(dummy_input)\n","\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.config.LEARNING_RATE)\n","        self.episode_rewards = []\n","\n","    def preprocess_frame(self, frame):\n","        frame = frame[1:176:2, ::2]\n","        return np.mean(frame, axis=-1, dtype=np.float32) / 255.0\n","\n","    def create_initial_state(self, frame):\n","        return np.stack([frame] * self.config.FRAME_STACK_SIZE, axis=-1)\n","\n","    def update_state(self, state, new_frame):\n","        return np.concatenate([state[..., 1:], np.expand_dims(new_frame, axis=-1)], axis=-1)\n","\n","    def collect_trajectories(self):\n","        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n","\n","        frame, _ = self.env.reset()\n","        frame = self.preprocess_frame(frame)\n","        state = self.create_initial_state(frame)\n","\n","        done = False\n","        while not done:\n","            policy_logits, value = self.policy_net(np.expand_dims(state, axis=0))\n","            action_probs = tf.nn.softmax(policy_logits)\n","            action = np.random.choice(self.action_size, p=action_probs.numpy()[0])\n","            log_prob = tf.math.log(action_probs[0, action])\n","\n","            next_frame, reward, done, _, _ = self.env.step(action)\n","            next_frame = self.preprocess_frame(next_frame)\n","            next_state = self.update_state(state, next_frame)\n","\n","            states.append(state)\n","            actions.append(action)\n","            rewards.append(reward)\n","            log_probs.append(log_prob)\n","            values.append(value.numpy()[0, 0])\n","            dones.append(done)\n","\n","            state = next_state\n","\n","        return states, actions, rewards, log_probs, values, dones\n","\n","    def compute_advantages(self, rewards, values, dones):\n","        advantages = np.zeros_like(rewards, dtype=np.float32)\n","        returns = np.zeros_like(rewards, dtype=np.float32)\n","\n","        last_advantage = 0\n","        for t in reversed(range(len(rewards))):\n","            delta = rewards[t] + (1 - dones[t]) * self.config.GAMMA * (values[t+1] if t+1 < len(values) else 0) - values[t]\n","            advantages[t] = last_advantage = delta + self.config.GAMMA * self.config.LAMBDA_GAE * (1 - dones[t]) * last_advantage\n","\n","        returns = advantages + np.array(values)\n","        return advantages, returns\n","\n","    def train(self):\n","        print(\"Starting training...\")\n","        start_time = time.time()\n","\n","        for episode in range(1, self.config.TOTAL_EPISODES + 1):\n","            episode_start = time.time()\n","            states, actions, rewards, log_probs, values, dones = self.collect_trajectories()\n","\n","            advantages, returns = self.compute_advantages(rewards, values, dones)\n","\n","            states = np.array(states)\n","            actions = np.array(actions)\n","            log_probs = np.array(log_probs, dtype=np.float32)\n","            returns = np.array(returns, dtype=np.float32)\n","            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n","\n","            for _ in range(self.config.UPDATE_EPOCHS):\n","                with tf.GradientTape() as tape:\n","                    policy_logits, new_values = self.policy_net(states)\n","                    new_probs = tf.nn.softmax(policy_logits)\n","                    new_log_probs = tf.math.log(tf.reduce_sum(new_probs * tf.one_hot(actions, self.action_size), axis=1))\n","\n","                    ratio = tf.exp(new_log_probs - log_probs)\n","                    clipped_ratio = tf.clip_by_value(ratio, 1 - self.config.EPS_CLIP, 1 + self.config.EPS_CLIP)\n","                    policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n","\n","                    value_loss = tf.keras.losses.MSE(returns, tf.squeeze(new_values))\n","\n","                    entropy = -tf.reduce_mean(new_probs * tf.math.log(new_probs + 1e-10))\n","                    loss = policy_loss + self.config.VF_COEF * value_loss - self.config.BETA_ENTROPY * entropy\n","\n","                grads = tape.gradient(loss, self.policy_net.trainable_variables)\n","                self.optimizer.apply_gradients(zip(grads, self.policy_net.trainable_variables))\n","\n","            total_reward = sum(rewards)\n","            self.episode_rewards.append(total_reward)\n","\n","            if episode % self.config.CHECKPOINT_FREQ == 0:\n","                self.save_checkpoint(episode)\n","                gc.collect()\n","\n","            if episode % 10 == 0 or episode == 1:\n","                avg_reward = np.mean(self.episode_rewards[-10:])\n","                print(f\"Ep {episode:4d} | R: {total_reward:6.1f} | Avg R: {avg_reward:6.1f}\")\n","\n","        self.save_checkpoint('final')\n","        print(f\"Training completed in {(time.time() - start_time)/60:.2f} minutes\")\n","\n","    def save_checkpoint(self, episode):\n","        path = os.path.join(self.config.CHECKPOINT_DIR, f\"ppo_pacman_ep{episode}.keras\")\n","        self.policy_net.save(path, include_optimizer=False)\n","        print(f\"Saved checkpoint: {path}\")\n","\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    config = TrainingConfig()\n","    agent = PPOAgent(config)\n","    agent.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9bc1qPktx6C","outputId":"9d2502a9-2190-459a-c7fc-d40aedd80b65","executionInfo":{"status":"ok","timestamp":1743860397770,"user_tz":-120,"elapsed":3696297,"user":{"displayName":"Ziad Sameh","userId":"01817499041981397063"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using configuration (RAM available: 11.0GB):\n","- Batch size: 32\n","- Frame stack: 3\n","Starting training...\n","Ep    1 | R:  190.0 | Avg R:  190.0\n","Ep   10 | R:  220.0 | Avg R:  287.0\n","Ep   20 | R:  220.0 | Avg R:  333.0\n","Ep   30 | R:  410.0 | Avg R:  347.0\n","Ep   40 | R:  590.0 | Avg R:  344.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep50.keras\n","Ep   50 | R:  740.0 | Avg R:  613.0\n","Ep   60 | R:  460.0 | Avg R:  334.0\n","Ep   70 | R:  190.0 | Avg R:  345.0\n","Ep   80 | R:  190.0 | Avg R:  392.0\n","Ep   90 | R:  190.0 | Avg R:  357.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep100.keras\n","Ep  100 | R:  230.0 | Avg R:  302.0\n","Ep  110 | R:  230.0 | Avg R:  276.0\n","Ep  120 | R:  180.0 | Avg R:  395.0\n","Ep  130 | R:  250.0 | Avg R:  414.0\n","Ep  140 | R:  230.0 | Avg R:  418.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep150.keras\n","Ep  150 | R:  380.0 | Avg R:  434.0\n","Ep  160 | R:  390.0 | Avg R:  473.0\n","Ep  170 | R:  240.0 | Avg R:  337.0\n","Ep  180 | R:  220.0 | Avg R:  332.0\n","Ep  190 | R:  200.0 | Avg R:  320.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep200.keras\n","Ep  200 | R:  220.0 | Avg R:  377.0\n","Ep  210 | R:  190.0 | Avg R:  383.0\n","Ep  220 | R:  290.0 | Avg R:  309.0\n","Ep  230 | R:  500.0 | Avg R:  366.0\n","Ep  240 | R:  460.0 | Avg R:  331.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep250.keras\n","Ep  250 | R:  260.0 | Avg R:  420.0\n","Ep  260 | R:  250.0 | Avg R:  443.0\n","Ep  270 | R:  190.0 | Avg R:  301.0\n","Ep  280 | R:  230.0 | Avg R:  307.0\n","Ep  290 | R:  310.0 | Avg R:  465.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep300.keras\n","Ep  300 | R:  240.0 | Avg R:  287.0\n","Ep  310 | R:  610.0 | Avg R:  542.0\n","Ep  320 | R:  450.0 | Avg R:  549.0\n","Ep  330 | R:  500.0 | Avg R:  383.0\n","Ep  340 | R:  480.0 | Avg R:  703.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep350.keras\n","Ep  350 | R:  580.0 | Avg R:  454.0\n","Ep  360 | R:  440.0 | Avg R:  616.0\n","Ep  370 | R:  440.0 | Avg R:  446.0\n","Ep  380 | R:  210.0 | Avg R:  302.0\n","Ep  390 | R:  810.0 | Avg R:  421.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep400.keras\n","Ep  400 | R:  440.0 | Avg R:  353.0\n","Ep  410 | R:  470.0 | Avg R:  396.0\n","Ep  420 | R:  390.0 | Avg R:  424.0\n","Ep  430 | R:  370.0 | Avg R:  456.0\n","Ep  440 | R:  180.0 | Avg R:  418.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep450.keras\n","Ep  450 | R:  430.0 | Avg R:  514.0\n","Ep  460 | R:  850.0 | Avg R:  383.0\n","Ep  470 | R:  390.0 | Avg R:  488.0\n","Ep  480 | R:  670.0 | Avg R:  561.0\n","Ep  490 | R:  840.0 | Avg R:  551.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep500.keras\n","Ep  500 | R:  410.0 | Avg R:  690.0\n","Ep  510 | R:  900.0 | Avg R:  378.0\n","Ep  520 | R:  530.0 | Avg R:  553.0\n","Ep  530 | R:  540.0 | Avg R:  502.0\n","Ep  540 | R:  520.0 | Avg R:  531.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep550.keras\n","Ep  550 | R:  310.0 | Avg R:  496.0\n","Ep  560 | R:  350.0 | Avg R:  638.0\n","Ep  570 | R:  940.0 | Avg R:  663.0\n","Ep  580 | R:  700.0 | Avg R:  589.0\n","Ep  590 | R:  740.0 | Avg R:  721.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep600.keras\n","Ep  600 | R:  410.0 | Avg R:  496.0\n","Ep  610 | R:  410.0 | Avg R:  558.0\n","Ep  620 | R:  560.0 | Avg R:  581.0\n","Ep  630 | R:  450.0 | Avg R:  607.0\n","Ep  640 | R: 1050.0 | Avg R:  814.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep650.keras\n","Ep  650 | R:  660.0 | Avg R:  590.0\n","Ep  660 | R:  420.0 | Avg R:  593.0\n","Ep  670 | R:  660.0 | Avg R:  733.0\n","Ep  680 | R:  380.0 | Avg R:  430.0\n","Ep  690 | R:  530.0 | Avg R:  384.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep700.keras\n","Ep  700 | R:  480.0 | Avg R:  337.0\n","Ep  710 | R:  330.0 | Avg R:  285.0\n","Ep  720 | R:  200.0 | Avg R:  301.0\n","Ep  730 | R:  250.0 | Avg R:  375.0\n","Ep  740 | R:  490.0 | Avg R:  308.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep750.keras\n","Ep  750 | R:  430.0 | Avg R:  418.0\n","Ep  760 | R:  150.0 | Avg R:  305.0\n","Ep  770 | R:  260.0 | Avg R:  311.0\n","Ep  780 | R:  500.0 | Avg R:  534.0\n","Ep  790 | R:  410.0 | Avg R:  732.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep800.keras\n","Ep  800 | R: 1230.0 | Avg R:  799.0\n","Ep  810 | R:  640.0 | Avg R:  659.0\n","Ep  820 | R:  360.0 | Avg R:  655.0\n","Ep  830 | R:  320.0 | Avg R:  574.0\n","Ep  840 | R: 1330.0 | Avg R:  598.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep850.keras\n","Ep  850 | R:  420.0 | Avg R:  701.0\n","Ep  860 | R:  150.0 | Avg R:  462.0\n","Ep  870 | R:  550.0 | Avg R:  633.0\n","Ep  880 | R:  850.0 | Avg R:  580.0\n","Ep  890 | R:  350.0 | Avg R:  458.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep900.keras\n","Ep  900 | R:  330.0 | Avg R:  457.0\n","Ep  910 | R:  510.0 | Avg R:  500.0\n","Ep  920 | R:  590.0 | Avg R:  498.0\n","Ep  930 | R:  580.0 | Avg R:  652.0\n","Ep  940 | R: 2240.0 | Avg R:  813.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep950.keras\n","Ep  950 | R:  380.0 | Avg R:  660.0\n","Ep  960 | R:  620.0 | Avg R:  603.0\n","Ep  970 | R:  340.0 | Avg R:  619.0\n","Ep  980 | R:  490.0 | Avg R:  482.0\n","Ep  990 | R:  630.0 | Avg R:  468.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_ep1000.keras\n","Ep 1000 | R:  690.0 | Avg R:  714.0\n","Saved checkpoint: /content/drive/MyDrive/ppo_pacman_models/ppo_pacman_epfinal.keras\n","Training completed in 123.31 minutes\n"]}]}]}